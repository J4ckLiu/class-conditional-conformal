{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "781b741c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "# import torch\n",
    "\n",
    "from collections import Counter\n",
    "from gap_statistic import OptimalK\n",
    "from scipy import stats, cluster\n",
    "from sklearn.cluster import KMeans\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "from utils.clustering_utils import *\n",
    "from utils.conformal_utils import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed7a956",
   "metadata": {},
   "source": [
    "## Testing null that there is one cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "491158c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics \n",
    "\n",
    "def _get_cluster_fit(true_class_scores, labels, num_classes, num_clusters):\n",
    "    \n",
    "    # Compute embeddings\n",
    "    q = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    embeddings = np.zeros((num_classes, len(q)))\n",
    "    for i in range(num_classes):\n",
    "        class_i_scores = true_class_scores[labels==i]\n",
    "        embeddings[i,:] = quantile_embedding(class_i_scores, q=q)\n",
    "\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=0, n_init=10).fit(embeddings)\n",
    "    \n",
    "# #     # OPTION 1: (Doesn't work well) Sum of squared distances of samples to their closest cluster center\n",
    "#     cluster_fit_metric = kmeans.inertia_\n",
    "\n",
    "      # OPTION 2: Silhouette score\n",
    "    cluster_labels = kmeans.labels_\n",
    "    cluster_fit_metric = metrics.silhouette_score(embeddings, cluster_labels, metric='euclidean')\n",
    "    \n",
    "    # OPTION 3: avg L1 distance of quantiles of original scores\n",
    "    # Group scores by cluster\n",
    "    list_of_cluster_scores = []\n",
    "    for i in range(num_clusters):\n",
    "        clusteri_classes = np.argwhere(cluster_labels == i)\n",
    "        clusteri_scores = true_class_scores[np.in1d(labels, clusteri_classes)]\n",
    "        list_of_cluster_scores.append(clusteri_scores)\n",
    "    cluster_fit_metric = compute_avg_distance_between_quantiles(list_of_cluster_scores, q=q)\n",
    "    \n",
    "    return cluster_fit_metric\n",
    "\n",
    "\n",
    "def test_one_cluster_null(scores, labels, num_classes, num_clusters=2, num_trials=100, seed=0):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    if len(scores.shape) > 1:\n",
    "        true_class_scores = get_true_class_conformal_score(scores, labels)\n",
    "    else:\n",
    "        true_class_scores = scores\n",
    "    \n",
    "    # Compute metric using true class labels\n",
    "    observed_metric = _get_cluster_fit(true_class_scores, labels, num_classes, num_clusters)   \n",
    "    \n",
    "    metrics_under_null = np.zeros((num_trials,))\n",
    "    permuted_labels = np.copy(labels)\n",
    "    for i in range(num_trials):\n",
    "        # Randomly permute labels\n",
    "        np.random.shuffle(permuted_labels)\n",
    "        \n",
    "        # Compute metric for each random permutation \n",
    "        metrics_under_null[i] = _get_cluster_fit(true_class_scores, permuted_labels, num_classes, num_clusters)\n",
    "        \n",
    "    # Compute fraction of results under null that yield a better clustering metric \n",
    "    # than the observed value \n",
    "    num_better = np.sum(metrics_under_null < observed_metric) # Lower inertia = better clustering\n",
    "    p_value = num_better / num_trials\n",
    "    \n",
    "    print('Observed metric:', observed_metric)\n",
    "    print('Metric under null:', metrics_under_null)\n",
    "    \n",
    "    print(f'Probability of observing a smaller metric under null hypothesis of one cluster: {p_value}',\n",
    "          f'({num_better} out of {num_trials} trials)')\n",
    "    \n",
    "    return p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d6134fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Version 2: operate directly on scores rather than embeddings [DOES NOT WORK]\n",
    "\n",
    "def compute_avg_distance_between_quantiles(list_of_arrs, q=[0.5, 0.6, 0.7, 0.8, 0.9]):\n",
    "    '''\n",
    "    Computes the L1 distance between quantiles q between each pair of groups\n",
    "    in list_of_arrs and then takes average across all pairs\n",
    "    \n",
    "    Input:\n",
    "        list_of_arrs: length-n list of arrays. list_of_arrs[i] contains \n",
    "        samples from group i\n",
    "    '''\n",
    "    n_groups = len(list_of_arrs)\n",
    "    \n",
    "    dists = []\n",
    "    for i in range(n_groups):\n",
    "        \n",
    "        groupi_quantiles = np.quantile(list_of_arrs[i], q)\n",
    "        \n",
    "        for j in range(i+1, n_groups):\n",
    "            groupj_quantiles = np.quantile(list_of_arrs[j], q)\n",
    "            \n",
    "            dist_ij = np.sum(np.abs(groupi_quantiles - groupj_quantiles))\n",
    "            dists.append(dist_ij)\n",
    "            \n",
    "    avg_dist = sum(dists) / len(dists)\n",
    "    return avg_dist\n",
    "\n",
    "# def cluster_and_group_scores_by_cluster(true_class_scores, labels, num_clusters):\n",
    "    \n",
    "#     # Compute embeddings\n",
    "#     q = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "#     embeddings = np.zeros((num_classes, len(q)))\n",
    "#     for i in range(num_classes):\n",
    "#         class_i_scores = true_class_scores[labels==i]\n",
    "#         embeddings[i,:] = quantile_embedding(class_i_scores, q=q)\n",
    "\n",
    "#     # Cluster\n",
    "#     kmeans = KMeans(n_clusters=num_clusters, random_state=0, n_init=10).fit(embeddings)\n",
    "#     cluster_labels = kmeans.labels_\n",
    "    \n",
    "#     # Group scores by cluster\n",
    "#     list_of_cluster_scores = []\n",
    "#     for i in range(num_clusters):\n",
    "#         clusteri_classes = np.argwhere(cluster_labels == i)\n",
    "#         clusteri_scores = true_class_scores[np.in1d(labels, clusteri_classes)]\n",
    "#         list_of_cluster_scores.append(clusteri_scores)\n",
    "        \n",
    "#     return list_of_cluster_scores \n",
    "\n",
    "# def test_one_cluster_null_v2(scores, labels, num_classes, num_clusters=2, num_trials=100, seed=0):\n",
    "#     np.random.seed(seed)\n",
    "#     q=[0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    \n",
    "#     if len(scores.shape) > 1:\n",
    "#         true_class_scores = get_true_class_conformal_score(scores, labels)\n",
    "#     else:\n",
    "#         true_class_scores = scores\n",
    "    \n",
    "#     # Compute metric using true class labels\n",
    "#     list_of_cluster_scores = cluster_and_group_scores_by_cluster(true_class_scores, labels, num_clusters)\n",
    "#     observed_metric = compute_avg_distance_between_quantiles(list_of_cluster_scores, q=q)\n",
    "    \n",
    "# #     print('OBS', list_of_cluster_scores)\n",
    "    \n",
    "#     metrics_under_null = np.zeros((num_trials,))\n",
    "#     for i in range(num_trials):\n",
    "#         # Split scores into num_clusters randomly sized chunks\n",
    "#         # - Randomly select a probability vector p\n",
    "#         unifs = np.random.rand(num_clusters)\n",
    "#         p = unifs / unifs.sum()\n",
    "        \n",
    "#         # - Assign classes based on p\n",
    "#         rand_clusters = np.random.choice(num_clusters, size=len(labels), p=p)\n",
    "#         list_of_cluster_scores = [true_class_scores[rand_clusters==i] for i in range(num_clusters)]\n",
    "        \n",
    "# #         print('SIM', list_of_cluster_scores)\n",
    "        \n",
    "#         # Compute metric for each random permutation \n",
    "#         metrics_under_null[i] = compute_avg_distance_between_quantiles(list_of_cluster_scores, q=q)\n",
    "        \n",
    "# #         assert False\n",
    "        \n",
    "#     # Compute fraction of results under null that yield a better clustering metric \n",
    "#     # than the observed value \n",
    "#     num_better = np.sum(metrics_under_null < observed_metric) # Lower inertia = better clustering\n",
    "#     p_value = num_better / num_trials\n",
    "    \n",
    "#     print('Observed metric:', observed_metric)\n",
    "#     print('Metric under null:', metrics_under_null)\n",
    "    \n",
    "#     print(f'Probability of observing a smaller metric under null hypothesis of one cluster: {p_value}',\n",
    "#           f'({num_better} out of {num_trials} trials)')\n",
    "    \n",
    "#     return p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe4aee9",
   "metadata": {},
   "source": [
    "### Test on Enron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e93a0888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "alpha = .1\n",
    "n_totalcal = 20 # Total number of calibration points (= # clustering examples + # conformal calibration examples)\n",
    "\n",
    "# Enron - BERT\n",
    "softmax_path = \"../class-conditional-conformal-datasets/notebooks/.cache/email_softmax_bert_ntrain=500.npy\"\n",
    "labels_path = \"../class-conditional-conformal-datasets/notebooks/.cache/email_labels_bert_ntrain=500.npy\"\n",
    "\n",
    "softmax_scores = np.load(softmax_path)\n",
    "labels = np.load(labels_path)\n",
    "\n",
    "num_classes = labels.max() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "48ef5097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed metric: 0.7757373115323073\n",
      "Metric under null: [0.45151132 0.32432147 0.73205709 0.49116746 0.23389451 0.43058253\n",
      " 0.39253624 0.46617533 0.30909889 0.80397174 0.67749834 0.82024545\n",
      " 0.70272406 0.51445724 0.29029255 0.35221266 0.57012928 0.90003647\n",
      " 0.44544204 0.27693843 0.3671603  0.45065448 1.53850399 0.41744018\n",
      " 0.30732166 0.39344402 0.44805636 0.41908243 0.69618777 1.26120738\n",
      " 0.33152527 0.76047233 0.36360908 0.62614049 1.09884185 1.90902774\n",
      " 0.30035567 1.11888252 0.72788612 0.42156505 0.35764796 1.29340075\n",
      " 0.34450692 0.98043791 0.45037955 1.12890606 0.27056832 0.36193183\n",
      " 0.21872629 0.29598139 0.54984367 0.41075872 0.72651938 0.34303703\n",
      " 0.36823209 0.64185557 0.29999708 0.42316498 0.61647996 0.51725537\n",
      " 0.32094597 0.39102757 0.61793278 0.40789829 0.32464459 0.57148852\n",
      " 0.41716768 0.4598939  0.60976683 0.54144907 0.29602576 0.98729637\n",
      " 0.46410569 0.36812143 0.31888791 0.40146241 0.38875357 0.44211959\n",
      " 0.78028015 0.32953237 0.33911119 0.44698013 0.54953297 0.25002406\n",
      " 0.31711084 0.23114181 0.33717718 0.30694045 0.43452559 0.30311271\n",
      " 1.67771519 0.26009442 0.34838794 0.25509212 0.5395814  0.57081523\n",
      " 0.60647098 1.1505655  0.29707011 0.4421768 ]\n",
      "Probability of observing a smaller metric under null hypothesis of one cluster: 0.85 (85 out of 100 trials)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.85"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_clustering = 5\n",
    "num_clusters = 3\n",
    "\n",
    "score_function = 'softmax'\n",
    "\n",
    "\n",
    "if score_function == 'softmax':\n",
    "    scores_all = 1 - softmax_scores\n",
    "    \n",
    "# Split into clustering+calibration data and validation data\n",
    "totalcal_scores_all, totalcal_labels, val_scores_all, val_labels = split_X_and_y(scores_all, labels, n_totalcal, num_classes=num_classes, seed=7)\n",
    "\n",
    "# 0) Split data between clustering and calibration\n",
    "scores1_all, labels1, scores2_all, labels2 = split_X_and_y(totalcal_scores_all, \n",
    "                                                       totalcal_labels, \n",
    "                                                       n_clustering, \n",
    "                                                       num_classes=num_classes, \n",
    "                                                       seed=0)\n",
    "\n",
    "# 1) Test k chosen using ad-hoc heuristic\n",
    "test_one_cluster_null(scores1_all, labels1, num_classes, num_clusters=num_clusters, num_trials=100, seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b1b5f0",
   "metadata": {},
   "source": [
    "### Synthetic test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e73eb6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed metric: 1.8425164414927768\n",
      "Metric under null: [0.86187347 0.87179594 0.9484072  0.97205673 0.79611811 1.02954443\n",
      " 0.89338773 0.71414115 0.98365402 1.04118377 0.99117361 0.78590661\n",
      " 1.01256282 1.15610226 0.81320829 0.95973893 0.8134263  1.00787498\n",
      " 0.95751718 0.85664703 0.87029901 1.17421971 1.30395335 0.93433334\n",
      " 1.00179653 0.94041709 0.92147795 1.04283627 0.8346762  0.8681899\n",
      " 0.90155756 1.05764202 1.00417968 0.92311172 0.98390793 0.76538461\n",
      " 0.84297992 0.90925041 1.2775974  0.90477097 0.92095417 0.9894978\n",
      " 1.06825953 0.90673281 0.98330046 1.21902753 0.94558885 1.03282373\n",
      " 0.79267238 1.12088129 0.96915478 0.84036883 1.10097703 0.94188114\n",
      " 1.00316437 1.40257299 0.86751182 0.96285074 0.96662509 1.217582\n",
      " 0.86403219 0.8022638  1.32255051 1.05810676 0.81474648 0.71010339\n",
      " 1.31229166 0.88728205 1.01444249 0.91783796 0.90669439 1.00289909\n",
      " 0.93147397 1.06774802 0.92460432 1.18133069 0.958657   1.13707261\n",
      " 1.02927718 1.05152956 0.94845884 0.79241753 0.84040538 0.82922434\n",
      " 0.94035698 0.98446083 0.92747149 0.80124557 1.11930379 0.88399532\n",
      " 1.14759099 0.78432832 1.10264553 0.98841298 1.03595456 0.70826839\n",
      " 1.02856352 0.90977322 0.78263781 0.95327695]\n",
      "Probability of observing a smaller metric under null hypothesis of one cluster: 1.0 (100 out of 100 trials)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate synthetic data (2 clusters of classes. One is Beta(1,1) and the other is Beta(3,.5))\n",
    "num_classes = 100\n",
    "n_clustering = 5\n",
    "\n",
    "true_class_scores = np.zeros((num_classes * n_clustering,))\n",
    "labels = np.zeros((num_classes * n_clustering,))\n",
    "for i in range(num_classes):\n",
    "    # Each class is in Cluster 0 or Cluster 1 with equal probability \n",
    "    if np.random.rand() > 0.5:\n",
    "#         samples = np.random.normal(0,1, size=n_clustering)\n",
    "        samples = np.random.beta(1,1, size=n_clustering)\n",
    "    else:\n",
    "#         samples = np.random.normal(0,1, size=n_clustering)\n",
    "        samples = np.random.beta(3,.5, size=n_clustering)\n",
    "        \n",
    "    true_class_scores[i*n_clustering:(i+1)*n_clustering] = samples \n",
    "        \n",
    "    labels[i*n_clustering:(i+1)*n_clustering] = i\n",
    "    \n",
    "# Test k=2 vs. k=1\n",
    "test_one_cluster_null(true_class_scores, labels, num_classes, num_clusters=2, num_trials=100, seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8bd2d2",
   "metadata": {},
   "source": [
    "Correct p-value? e.g., seed=5 gives p=.11 but other times it is < .03. Should maybe increase num_trials beyond 100 for more stability\n",
    "\n",
    "[silhouette] With 1000 trials and 100 classes, Beta (1,1), Beta(3, .5). Higher score = better cluster\n",
    "\n",
    "P(lower) is \n",
    "* 0.986\n",
    "* 1.0\n",
    "* 0.963\n",
    "* 0.992\n",
    "\n",
    "[inertia] With 1000 trials and 100 classes, Beta (1,1), Beta(3, .5). Lower score = better cluster. But does not account for the fact that randomizing results in embeddings that are closer together overall\n",
    "\n",
    "P(lower) is \n",
    "* 0.998 but we actually want this to be close to 0!!!\n",
    "\n",
    "\n",
    "[inertia] With 100 trials and 100 classes, Normal(0,1), Normal(10,1). \n",
    "P(lower is)\n",
    "* 0.0\n",
    "\n",
    "\n",
    "In conclusion, silhouette is better for our use case because it is standardized. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c708f1b",
   "metadata": {},
   "source": [
    "### Test on ImageNet - softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e2f35c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "alpha = .1\n",
    "n_totalcal = 10 # Total number of calibration points (= # clustering examples + # conformal calibration examples)\n",
    "\n",
    "# ImageNet\n",
    "softmax_path = '/home/tding/data/finetuned_imagenet/imagenet_train_subset_softmax.npy'\n",
    "labels_path = '/home/tding/data/finetuned_imagenet/imagenet_train_subset_labels.npy'\n",
    "softmax_scores = np.load(softmax_path)\n",
    "labels = np.load(labels_path)\n",
    "\n",
    "num_classes = labels.max() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f473681c",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_function = 'softmax'\n",
    "\n",
    "\n",
    "if score_function == 'softmax':\n",
    "    scores_all = 1 - softmax_scores\n",
    "    \n",
    "# Split into clustering+calibration data and validation data\n",
    "totalcal_scores_all, totalcal_labels, val_scores_all, val_labels = split_X_and_y(scores_all, labels, n_totalcal, num_classes=num_classes, seed=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bf98a2d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed metric: 1.5014748257909503\n",
      "Metric under null: [1.34337364 1.34823567 1.35910352 1.33588652 1.30077732 1.35194303\n",
      " 1.34619584 1.38380463 1.40494304 1.34793517 1.2848282  1.35615271\n",
      " 1.33192835 1.36943094 1.31527481 1.37436384 1.34828231 1.32104032\n",
      " 1.36042272 1.37184055 1.34199021 1.31849631 1.37028395 1.327778\n",
      " 1.3274847  1.35686893 1.33207267 1.36747525 1.33284109 1.34138328\n",
      " 1.3834615  1.31482878 1.39302892 1.31821949 1.35974126 1.34644293\n",
      " 1.3061125  1.33753944 1.31653484 1.36780583 1.30645252 1.34614379\n",
      " 1.34176885 1.28651441 1.35002671 1.28216354 1.34035147 1.32556597\n",
      " 1.31701139 1.33371759 1.35105503 1.35966238 1.28096273 1.35130207\n",
      " 1.32273301 1.37076126 1.30632771 1.39565656 1.30777287 1.30715527\n",
      " 1.36965516 1.36329009 1.33515404 1.36878446 1.36936872 1.34348023\n",
      " 1.32248072 1.34989239 1.3394125  1.36238889 1.33879649 1.32879162\n",
      " 1.34444194 1.29470788 1.31599475 1.38324171 1.37345336 1.38477182\n",
      " 1.35027722 1.32980355 1.32493293 1.40223242 1.34168502 1.36346118\n",
      " 1.3420812  1.36616765 1.3529801  1.32087542 1.34545059 1.31136395\n",
      " 1.32037337 1.3483635  1.33927003 1.36218515 1.30149403 1.35585244\n",
      " 1.32528292 1.3193991  1.36558547 1.34503984]\n",
      "Probability of observing a smaller metric under null hypothesis of one cluster: 1.0 (100 out of 100 trials)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_clustering = 9\n",
    "num_clusters = 50\n",
    "\n",
    "# 0) Split data between clustering and calibration\n",
    "scores1_all, labels1, scores2_all, labels2 = split_X_and_y(totalcal_scores_all, \n",
    "                                                       totalcal_labels, \n",
    "                                                       n_clustering, \n",
    "                                                       num_classes=num_classes, \n",
    "                                                       seed=0)\n",
    "\n",
    "# 1) Test k chosen using ad-hoc heuristic\n",
    "test_one_cluster_null(scores1_all, labels1, num_classes, num_clusters=num_clusters, num_trials=100, seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b76dc6",
   "metadata": {},
   "source": [
    "### Test on ImageNet - APS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ad2feea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "alpha = .1\n",
    "n_totalcal = 10 # Total number of calibration points (= # clustering examples + # conformal calibration examples)\n",
    "\n",
    "# ImageNet\n",
    "APS_path = '/home/tding/data/finetuned_imagenet/imagenet_train_subset_APS.npy'\n",
    "labels_path = '/home/tding/data/finetuned_imagenet/imagenet_train_subset_labels.npy'\n",
    "APS_scores = np.load(APS_path)\n",
    "labels = np.load(labels_path)\n",
    "\n",
    "num_classes = labels.max() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "abf688c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_all = APS_scores\n",
    "\n",
    "# Split into clustering+calibration data and validation data\n",
    "totalcal_scores_all, totalcal_labels, val_scores_all, val_labels = split_X_and_y(scores_all, labels, n_totalcal, num_classes=num_classes, seed=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2af3e1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed metric: 1.2852707739201041\n",
      "Metric under null: [1.14475778 1.08360021 1.10488874 1.08136564 1.05255599 1.13209695\n",
      " 1.07458268 1.08542867 1.24344913 1.08505069 1.14587061 1.19985392\n",
      " 1.26749752 1.08098534 1.07252618 1.30788533 1.17926092 1.15843491\n",
      " 1.13979329 1.11182036 1.25768946 1.07926848 1.11760908 1.21430416\n",
      " 1.10692022 1.13669431 1.11049796 1.16183259 1.14150135 1.11540434\n",
      " 1.14976526 1.09699924 1.12444508 1.12418124 1.24842408 1.14833131\n",
      " 1.09286472 1.22571067 1.11569564 1.13573816 1.29479786 1.1317263\n",
      " 1.11881283 1.10208476 1.1365222  1.21136059 1.08514501 1.13615582\n",
      " 1.14465668 1.15464564 1.18747472 1.13685776 1.23081858 1.12058058\n",
      " 1.11648811 1.12578767 1.15818624 1.092296   1.2192506  1.0831188\n",
      " 1.09661343 1.28469823 1.11335996 1.07396858 1.09910141 1.09966898\n",
      " 1.23482948 1.10342615 1.10705648 1.12493029 1.20657005 1.16475902\n",
      " 1.23133083 1.07055279 1.05491229 1.30033174 1.09313835 1.07284801\n",
      " 1.10334683 1.28130213 1.26155882 1.13898708 1.14907569 1.29483329\n",
      " 1.0708185  1.11120454 1.13784825 1.12017013 1.13152906 1.13141937\n",
      " 1.23737011 1.13916535 1.12951583 1.22893154 1.09138013 1.1031675\n",
      " 1.06368083 1.27864488 1.08907428 1.09727313 1.18711332 1.08591998\n",
      " 1.15378297 1.11455615 1.20210577 1.05242135 1.08674808 1.21708076\n",
      " 1.1421869  1.04616614 1.10583128 1.11358397 1.11593192 1.091008\n",
      " 1.10807214 1.28368707 1.24364055 1.09561403 1.08532313 1.12820596\n",
      " 1.10887755 1.18573982 1.13150986 1.08252297 1.27216028 1.08492506\n",
      " 1.18872153 1.14681463 1.29919426 1.15092293 1.0769594  1.27081605\n",
      " 1.1913211  1.12140681 1.0799491  1.12886531 1.1434461  1.1283979\n",
      " 1.27052978 1.21315369 1.08472853 1.09677903 1.07690727 1.23624315\n",
      " 1.06453133 1.11400196 1.1313957  1.08487935 1.14347701 1.08191897\n",
      " 1.10725213 1.11738476 1.15707003 1.08262993 1.11721045 1.17273505\n",
      " 1.12421827 1.12612738 1.11263041 1.1226159  1.07180902 1.12206069\n",
      " 1.22256213 1.18875359 1.12350196 1.17664229 1.18727404 1.10055587\n",
      " 1.09791345 1.30279028 1.13162153 1.06952045 1.12053624 1.15757554\n",
      " 1.17066316 1.30213428 1.18157001 1.16251665 1.13258231 1.16477227\n",
      " 1.22363674 1.28347895 1.22381139 1.08448583 1.12942276 1.31511504\n",
      " 1.13049607 1.34222354 1.12290091 1.1031686  1.11260224 1.14811065\n",
      " 1.08634945 1.27043768 1.05328033 1.09535307 1.33347638 1.10841761\n",
      " 1.12232201 1.09792279 1.3123719  1.17784789 1.1407649  1.26268227\n",
      " 1.14551329 1.06378491 1.18566067 1.14142062 1.06658705 1.13990613\n",
      " 1.15280919 1.14813869 1.08474866 1.06812294 1.16217934 1.11104702\n",
      " 1.19646382 1.10343586 1.10183329 1.21760203 1.10485313 1.14814601\n",
      " 1.14139435 1.11244145 1.11699153 1.27718106 1.13733461 1.15269831\n",
      " 1.09205157 1.33541489 1.12709363 1.11936123 1.10401647 1.13340635\n",
      " 1.13847614 1.19762002 1.08488798 1.04775586 1.14785358 1.21470846\n",
      " 1.10372427 1.09844488 1.13773769 1.25331511 1.13612939 1.21846354\n",
      " 1.26981949 1.14185781 1.08644993 1.0934936  1.09634069 1.17396426\n",
      " 1.08906203 1.20327781 1.12034333 1.10279633 1.2133285  1.08630608\n",
      " 1.10672963 1.09041971 1.04963668 1.07942023 1.09382568 1.12586176\n",
      " 1.1540827  1.10065752 1.05772907 1.19589559 1.09153389 1.05433046\n",
      " 1.17645415 1.06257029 1.1910681  1.1181915  1.18577532 1.14178402\n",
      " 1.26861014 1.13266358 1.07687797 1.21171834 1.1166848  1.12911674\n",
      " 1.13756323 1.1643069  1.27813823 1.1934475  1.3092983  1.14330337\n",
      " 1.09813915 1.26271072 1.10402889 1.10797502 1.11548075 1.10725902\n",
      " 1.11978673 1.07532021 1.1325461  1.10204235 1.10876291 1.10569614\n",
      " 1.11646343 1.12265531 1.13573302 1.06257583 1.12625768 1.15700251\n",
      " 1.09754254 1.20753204 1.23164406 1.25233901 1.13010908 1.11849535\n",
      " 1.14855912 1.29637182 1.07501527 1.132139   1.13847625 1.11723425\n",
      " 1.10596964 1.15037273 1.24623192 1.07593047 1.09310728 1.16836585\n",
      " 1.18726111 1.24073772 1.23763035 1.12408855 1.29840228 1.28570991\n",
      " 1.07030363 1.19083563 1.11837409 1.12740923 1.1190059  1.09158278\n",
      " 1.10662855 1.35173276 1.11586201 1.15828533 1.14200297 1.13307666\n",
      " 1.27647733 1.14505781 1.14571428 1.11441378 1.18424747 1.23111053\n",
      " 1.26128261 1.14721916 1.12532561 1.14723856 1.06102693 1.20447249\n",
      " 1.15216936 1.12645701 1.16350182 1.07447087 1.08735476 1.14473299\n",
      " 1.09683804 1.08340894 1.12100642 1.14683648 1.25041741 1.11848261\n",
      " 1.32787353 1.07708318 1.2434991  1.27673181 1.14464904 1.18968165\n",
      " 1.13383789 1.10737846 1.11269207 1.08055077 1.17288658 1.14468437\n",
      " 1.03986929 1.24547213 1.12035027 1.19913538 1.129005   1.28426329\n",
      " 1.29407063 1.09026225 1.07970552 1.0774769  1.18348189 1.11010699\n",
      " 1.18522235 1.21646616 1.17042807 1.13881722 1.22656126 1.15756795\n",
      " 1.0955498  1.08835059 1.10048294 1.08785665 1.29429123 1.12601192\n",
      " 1.16333878 1.077925   1.25575176 1.21694041 1.08897649 1.2555542\n",
      " 1.10627782 1.14801255 1.18151478 1.28850037 1.08213066 1.10498166\n",
      " 1.12374956 1.09296388 1.13326788 1.09953971 1.17149656 1.25524839\n",
      " 1.10160801 1.11034324 1.10561318 1.16385282 1.09293503 1.13627737\n",
      " 1.06379228 1.15160963 1.16596941 1.11403409 1.15247826 1.07857703\n",
      " 1.145959   1.08616337 1.2664521  1.10543996 1.11296924 1.15784941\n",
      " 1.11936818 1.14992113 1.13350573 1.21223306 1.13320486 1.128031\n",
      " 1.0900371  1.16996512 1.10654985 1.1112694  1.21170559 1.13925117\n",
      " 1.26048365 1.19861978 1.1462283  1.11248467 1.11431171 1.11196313\n",
      " 1.12755586 1.05520163 1.11671732 1.22050908 1.0803355  1.21422445\n",
      " 1.14661621 1.19570214 1.23100137 1.09224382 1.17374331 1.24580826\n",
      " 1.13089307 1.1262443  1.11988189 1.15156508 1.0945363  1.23793159\n",
      " 1.12871021 1.13850186 1.15524516 1.11218439 1.13464632 1.10598458\n",
      " 1.13503021 1.2746705  1.13591182 1.13255336 1.25168617 1.22797104\n",
      " 1.27088457 1.20868464 1.24054651 1.24185813 1.10390517 1.2558084\n",
      " 1.06241952 1.16084662 1.32526299 1.08219807 1.09492412 1.29457935\n",
      " 1.17371976 1.09255633 1.11571056 1.08077876 1.12215669 1.13568078\n",
      " 1.29675169 1.10061529 1.18339468 1.11804837 1.26878453 1.02352421\n",
      " 1.09158315 1.1046382  1.24750164 1.11019706 1.10284162 1.21416955\n",
      " 1.13912784 1.09449497 1.23230572 1.06240844 1.27661651 1.17900375\n",
      " 1.32552286 1.23255079 1.13390455 1.15928568 1.14060834 1.16256292\n",
      " 1.13572586 1.22669503 1.11170558 1.20158496 1.11609698 1.12483141\n",
      " 1.24386975 1.14991998 1.11536271 1.27806564 1.12192629 1.16915354\n",
      " 1.12722734 1.28067942 1.11010062 1.09830222 1.11027013 1.15551457\n",
      " 1.11928425 1.14075102 1.15675469 1.08529566 1.1113913  1.12074872\n",
      " 1.17783183 1.22814963 1.06625343 1.09761759 1.22352807 1.13518975\n",
      " 1.0934723  1.24992047 1.15280561 1.13721482 1.15353351 1.16937443\n",
      " 1.13534727 1.31899645 1.1057384  1.11528955 1.15369215 1.09631167\n",
      " 1.23132813 1.07114967 1.23276655 1.15096302 1.15003366 1.06813053\n",
      " 1.25658261 1.07634751 1.11234441 1.15346389 1.09507684 1.2373\n",
      " 1.24958658 1.17196745 1.0827264  1.05348443 1.08738672 1.21188458\n",
      " 1.0858721  1.09394994 1.11233358 1.1076139  1.15451117 1.12145741\n",
      " 1.13467559 1.12642371 1.0903185  1.12489963 1.1287661  1.11677705\n",
      " 1.15355958 1.12459993 1.19782339 1.14314472 1.13595571 1.12514848\n",
      " 1.20222803 1.06726774 1.13821779 1.32231319 1.09153366 1.12771802\n",
      " 1.25855434 1.11201156 1.13697584 1.24487052 1.12049228 1.10964963\n",
      " 1.10003427 1.11434349 1.26276462 1.23248171 1.10590741 1.13790557\n",
      " 1.10168226 1.11279508 1.11092314 1.11552419 1.23823464 1.29148386\n",
      " 1.12895811 1.13453131 1.10813333 1.1815179  1.22347875 1.07305665\n",
      " 1.0746127  1.13559483 1.15254104 1.11171643 1.13638031 1.11837766\n",
      " 1.10612275 1.35313851 1.07957523 1.08640936 1.12278437 1.11902002\n",
      " 1.09567264 1.07923311 1.10925435 1.07775131 1.24664137 1.12031765\n",
      " 1.25759045 1.12526436 1.13571858 1.06116447 1.27630493 1.14963814\n",
      " 1.13094682 1.2152997  1.11180278 1.09825907 1.14263149 1.17320639\n",
      " 1.08345783 1.11715647 1.08169796 1.25166754 1.11033149 1.27020838\n",
      " 1.1399006  1.11125418 1.21542493 1.09262766 1.08104319 1.10137145\n",
      " 1.14775123 1.10277939 1.10823007 1.17200626 1.13785869 1.1771754\n",
      " 1.15318261 1.13673    1.26930723 1.28901281 1.1249217  1.1576745\n",
      " 1.1214784  1.09268579 1.11479212 1.08709946 1.03647692 1.16801684\n",
      " 1.07461193 1.08365714 1.13993482 1.32173718 1.11511346 1.09481722\n",
      " 1.16330103 1.14263564 1.07068564 1.27383719 1.08350831 1.11521689\n",
      " 1.11240343 1.13020952 1.08773121 1.08792251 1.16571992 1.11326003\n",
      " 1.12726715 1.2798945  1.1814193  1.20426453 1.26282998 1.12089652\n",
      " 1.10386433 1.21606156 1.16906683 1.15107458 1.22486798 1.23255162\n",
      " 1.07419701 1.13524485 1.09447523 1.1127352  1.10961696 1.21097317\n",
      " 1.16190356 1.16264398 1.15992835 1.14629288 1.01906718 1.08991567\n",
      " 1.24155452 1.16524011 1.30978622 1.16645744 1.1087903  1.2314386\n",
      " 1.12074245 1.18377287 1.06707472 1.30989065 1.13901274 1.11151153\n",
      " 1.36484761 1.23709242 1.19549522 1.1091579  1.15088924 1.25962826\n",
      " 1.13348491 1.14292703 1.12645138 1.06066281 1.05631779 1.09296537\n",
      " 1.05355634 1.10934397 1.12122552 1.16017568 1.14310396 1.16198702\n",
      " 1.30697627 1.09825148 1.0777612  1.04484995 1.13481866 1.16279779\n",
      " 1.23053901 1.08749317 1.06646571 1.11460223 1.22298202 1.08843064\n",
      " 1.16735248 1.12487486 1.153852   1.16461381 1.31774532 1.29689696\n",
      " 1.1022543  1.11440476 1.04556993 1.07995922 1.05495447 1.16086705\n",
      " 1.13685235 1.09869236 1.15606474 1.21613004 1.07062456 1.12832729\n",
      " 1.12134905 1.11823141 1.28416025 1.10096394 1.12576139 1.26614137\n",
      " 1.09405262 1.14419325 1.07367115 1.24968499 1.27023502 1.26422702\n",
      " 1.09294939 1.25627179 1.12007771 1.13411898 1.09976193 1.17094795\n",
      " 1.29424393 1.30197116 1.14589997 1.06674436 1.15908218 1.10840472\n",
      " 1.15828064 1.16675469 1.30272242 1.12089143 1.12913082 1.11644213\n",
      " 1.19497737 1.17606756 1.07414323 1.08765232 1.11938571 1.29428164\n",
      " 1.13271211 1.10590278 1.09442698 1.13417013 1.07812434 1.23734634\n",
      " 1.15849289 1.14878991 1.12579617 1.11607868 1.12938415 1.10714109\n",
      " 1.24767356 1.12957067 1.15572194 1.22883984 1.10160965 1.12961703\n",
      " 1.13487385 1.12143738 1.24922923 1.12675712 1.12762503 1.11859993\n",
      " 1.15029319 1.09378807 1.17448218 1.20226238 1.12140491 1.10300861\n",
      " 1.26343008 1.03872848 1.08395707 1.11296984 1.25348059 1.09696555\n",
      " 1.15183776 1.16578188 1.14731494 1.07365489 1.26444456 1.19282784\n",
      " 1.10368743 1.14915172 1.12148529 1.12069574 1.08627598 1.1433383\n",
      " 1.28834716 1.27964988 1.07275475 1.20358744 1.07072571 1.21433817\n",
      " 1.14447919 1.25459867 1.11529078 1.16700123 1.10515523 1.08749039\n",
      " 1.28260883 1.22522017 1.10438818 1.11163881 1.23800546 1.21427118\n",
      " 1.24077636 1.14958718 1.0928755  1.13141094 1.14328023 1.05283641\n",
      " 1.17983947 1.12255554 1.10817553 1.10116026 1.29471524 1.18178246\n",
      " 1.12688038 1.18085597 1.07589213 1.18814134 1.09243439 1.06626356\n",
      " 1.2054832  1.23525974 1.11817838 1.16722743 1.15751526 1.13881284\n",
      " 1.10953314 1.11626176 1.13499109 1.09754081 1.11140378 1.21200804\n",
      " 1.2164618  1.19002098 1.09955454 1.13425961 1.11624233 1.24935237\n",
      " 1.0905593  1.28104643 1.2836769  1.27917365 1.13707886 1.12366537\n",
      " 1.12468761 1.13117207 1.09458301 1.09017109 1.10465504 1.11165575\n",
      " 1.16150412 1.1367399  1.07814789 1.09577315 1.07805432 1.26695616\n",
      " 1.12334974 1.12739676 1.08596421 1.12720008 1.10364799 1.31255979\n",
      " 1.17106329 1.10282122 1.20196422 1.18010622 1.13163378 1.06021048\n",
      " 1.23494603 1.15560964 1.13322206 1.06306843 1.11144949 1.13095167\n",
      " 1.11366229 1.2993033  1.10909302 1.10862874 1.14897513 1.11402966\n",
      " 1.15938689 1.12339192 1.11923927 1.17644706 1.19856187 1.10325473\n",
      " 1.21549217 1.13285813 1.11682622 1.15283055 1.05288526 1.12958292\n",
      " 1.2614334  1.05697285 1.11328713 1.09547066 1.06097429 1.13089963\n",
      " 1.11211952 1.30815981 1.1563077  1.08478151]\n",
      "Probability of observing a smaller metric under null hypothesis of one cluster: 0.954 (954 out of 1000 trials)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.954"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_clustering = 5\n",
    "num_clusters = 5\n",
    "\n",
    "# 0) Split data between clustering and calibration\n",
    "scores1_all, labels1, scores2_all, labels2 = split_X_and_y(totalcal_scores_all, \n",
    "                                                       totalcal_labels, \n",
    "                                                       n_clustering, \n",
    "                                                       num_classes=num_classes, \n",
    "                                                       seed=0)\n",
    "\n",
    "# 1) Test k chosen using ad-hoc heuristic\n",
    "test_one_cluster_null(scores1_all, labels1, num_classes, num_clusters=num_clusters, num_trials=1000, seed=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac97b0e2",
   "metadata": {},
   "source": [
    "n_clustering = 5\n",
    "num_clusters = 5\n",
    "\n",
    "seed=0\n",
    "Probability of observing a smaller metric under null hypothesis of one cluster: 0.949 (949 out of 1000 trials)\n",
    "seed=1\n",
    "Probability of observing a smaller metric under null hypothesis of one cluster: 0.949 (949 out of 1000 trials)\n",
    "seed=2\n",
    "Probability of observing a smaller metric under null hypothesis of one cluster: 0.954 (954 out of 1000 trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d663ee",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6dc2cb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = .1\n",
    "n_totalcal = 20 # Total number of calibration points (= # clustering examples + # conformal calibration examples)\n",
    "\n",
    "\n",
    "# Enron - BERT\n",
    "softmax_path = \"../class-conditional-conformal-datasets/notebooks/.cache/email_softmax_bert.npy\"\n",
    "labels_path = \"../class-conditional-conformal-datasets/notebooks/.cache/email_labels_bert.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b47275fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading softmax scores and labels...\n"
     ]
    }
   ],
   "source": [
    "## 1. Get data ============================\n",
    "print('Loading softmax scores and labels...')\n",
    "\n",
    "softmax_scores = np.load(softmax_path)\n",
    "labels = np.load(labels_path)\n",
    "\n",
    "num_classes = labels.max() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e18cd4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== score_function=softmax ======\n",
      "Computing conformal score...\n",
      "Splitting data...\n",
      "[n_clustering=3, num_clusters=1] Cluster sizes: [109]\n",
      "[n_clustering=3, num_clusters=2] Cluster sizes: [87, 22]\n",
      "[n_clustering=3, num_clusters=3] Cluster sizes: [74, 28, 7]\n",
      "[n_clustering=3, num_clusters=4] Cluster sizes: [62, 26, 17, 4]\n",
      "[n_clustering=3, num_clusters=5] Cluster sizes: [61, 26, 14, 6, 2]\n",
      "[n_clustering=3, num_clusters=6] Cluster sizes: [59, 22, 17, 5, 4, 2]\n",
      "[n_clustering=3, num_clusters=7] Cluster sizes: [59, 16, 13, 13, 4, 2, 2]\n",
      "[n_clustering=3, num_clusters=8] Cluster sizes: [59, 16, 13, 13, 3, 2, 2, 1]\n",
      "[n_clustering=3, num_clusters=9] Cluster sizes: [59, 16, 12, 11, 3, 3, 2, 2, 1]\n",
      "[n_clustering=3, num_clusters=10] Cluster sizes: [36, 23, 16, 12, 11, 3, 3, 2, 2, 1]\n",
      "[n_clustering=3, num_clusters=11] Cluster sizes: [31, 28, 16, 11, 8, 4, 3, 3, 2, 2, 1]\n",
      "[n_clustering=3, num_clusters=12] Cluster sizes: [36, 23, 15, 12, 7, 3, 3, 3, 2, 2, 2, 1]\n",
      "[n_clustering=5, num_clusters=1] Cluster sizes: [109]\n",
      "[n_clustering=5, num_clusters=2] Cluster sizes: [91, 18]\n",
      "[n_clustering=5, num_clusters=3] Cluster sizes: [88, 15, 6]\n",
      "[n_clustering=5, num_clusters=4] Cluster sizes: [69, 28, 8, 4]\n",
      "[n_clustering=5, num_clusters=5] Cluster sizes: [68, 28, 7, 4, 2]\n",
      "[n_clustering=5, num_clusters=6] Cluster sizes: [60, 29, 8, 7, 3, 2]\n",
      "[n_clustering=5, num_clusters=7] Cluster sizes: [57, 18, 14, 8, 7, 3, 2]\n",
      "[n_clustering=5, num_clusters=8] Cluster sizes: [50, 20, 19, 6, 5, 4, 3, 2]\n",
      "[n_clustering=5, num_clusters=9] Cluster sizes: [46, 22, 18, 6, 5, 4, 3, 3, 2]\n",
      "[n_clustering=5, num_clusters=10] Cluster sizes: [56, 17, 16, 7, 4, 3, 2, 2, 1, 1]\n",
      "[n_clustering=5, num_clusters=11] Cluster sizes: [50, 16, 14, 8, 4, 3, 3, 3, 3, 3, 2]\n",
      "[n_clustering=5, num_clusters=12] Cluster sizes: [33, 25, 16, 15, 4, 4, 3, 3, 2, 2, 1, 1]\n",
      "[n_clustering=7, num_clusters=1] Cluster sizes: [109]\n",
      "[n_clustering=7, num_clusters=2] Cluster sizes: [97, 12]\n",
      "[n_clustering=7, num_clusters=3] Cluster sizes: [93, 14, 2]\n",
      "[n_clustering=7, num_clusters=4] Cluster sizes: [70, 27, 11, 1]\n",
      "[n_clustering=7, num_clusters=5] Cluster sizes: [70, 27, 8, 3, 1]\n",
      "[n_clustering=7, num_clusters=6] Cluster sizes: [69, 25, 9, 3, 2, 1]\n",
      "[n_clustering=7, num_clusters=7] Cluster sizes: [67, 22, 8, 6, 3, 2, 1]\n",
      "[n_clustering=7, num_clusters=8] Cluster sizes: [67, 22, 8, 4, 3, 3, 1, 1]\n",
      "[n_clustering=7, num_clusters=9] Cluster sizes: [42, 27, 20, 8, 4, 3, 3, 1, 1]\n",
      "[n_clustering=7, num_clusters=10] Cluster sizes: [42, 27, 21, 7, 3, 3, 3, 1, 1, 1]\n",
      "[n_clustering=7, num_clusters=11] Cluster sizes: [45, 25, 20, 7, 3, 3, 2, 1, 1, 1, 1]\n",
      "[n_clustering=7, num_clusters=12] Cluster sizes: [42, 27, 19, 6, 3, 3, 3, 2, 1, 1, 1, 1]\n",
      "[n_clustering=9, num_clusters=1] Cluster sizes: [109]\n",
      "[n_clustering=9, num_clusters=2] Cluster sizes: [92, 17]\n",
      "[n_clustering=9, num_clusters=3] Cluster sizes: [92, 16, 1]\n",
      "[n_clustering=9, num_clusters=4] Cluster sizes: [88, 17, 3, 1]\n",
      "[n_clustering=9, num_clusters=5] Cluster sizes: [85, 11, 9, 3, 1]\n",
      "[n_clustering=9, num_clusters=6] Cluster sizes: [59, 29, 9, 8, 3, 1]\n",
      "[n_clustering=9, num_clusters=7] Cluster sizes: [58, 29, 9, 6, 3, 3, 1]\n",
      "[n_clustering=9, num_clusters=8] Cluster sizes: [58, 29, 9, 5, 3, 2, 2, 1]\n",
      "[n_clustering=9, num_clusters=9] Cluster sizes: [53, 32, 7, 5, 4, 3, 2, 2, 1]\n",
      "[n_clustering=9, num_clusters=10] Cluster sizes: [58, 29, 5, 5, 4, 3, 2, 1, 1, 1]\n",
      "[n_clustering=9, num_clusters=11] Cluster sizes: [53, 28, 7, 5, 4, 4, 3, 2, 1, 1, 1]\n",
      "[n_clustering=9, num_clusters=12] Cluster sizes: [53, 29, 6, 4, 4, 4, 3, 2, 1, 1, 1, 1]\n",
      "====== score_function=APS ======\n",
      "Computing conformal score...\n",
      "Splitting data...\n",
      "[n_clustering=3, num_clusters=1] Cluster sizes: [109]\n",
      "[n_clustering=3, num_clusters=2] Cluster sizes: [63, 46]\n",
      "[n_clustering=3, num_clusters=3] Cluster sizes: [44, 37, 28]\n",
      "[n_clustering=3, num_clusters=4] Cluster sizes: [32, 27, 26, 24]\n",
      "[n_clustering=3, num_clusters=5] Cluster sizes: [29, 25, 23, 18, 14]\n",
      "[n_clustering=3, num_clusters=6] Cluster sizes: [26, 25, 18, 17, 12, 11]\n",
      "[n_clustering=3, num_clusters=7] Cluster sizes: [22, 19, 17, 16, 14, 12, 9]\n",
      "[n_clustering=3, num_clusters=8] Cluster sizes: [19, 19, 18, 16, 12, 12, 7, 6]\n",
      "[n_clustering=3, num_clusters=9] Cluster sizes: [19, 18, 16, 14, 12, 11, 7, 7, 5]\n",
      "[n_clustering=3, num_clusters=10] Cluster sizes: [20, 17, 12, 11, 10, 9, 9, 9, 7, 5]\n",
      "[n_clustering=3, num_clusters=11] Cluster sizes: [19, 12, 12, 11, 11, 9, 9, 8, 7, 6, 5]\n",
      "[n_clustering=3, num_clusters=12] Cluster sizes: [17, 13, 13, 13, 9, 9, 7, 7, 6, 5, 5, 5]\n",
      "[n_clustering=5, num_clusters=1] Cluster sizes: [109]\n",
      "[n_clustering=5, num_clusters=2] Cluster sizes: [66, 43]\n",
      "[n_clustering=5, num_clusters=3] Cluster sizes: [39, 37, 33]\n",
      "[n_clustering=5, num_clusters=4] Cluster sizes: [35, 34, 29, 11]\n",
      "[n_clustering=5, num_clusters=5] Cluster sizes: [29, 25, 23, 21, 11]\n",
      "[n_clustering=5, num_clusters=6] Cluster sizes: [28, 22, 21, 14, 13, 11]\n",
      "[n_clustering=5, num_clusters=7] Cluster sizes: [20, 20, 19, 19, 11, 11, 9]\n",
      "[n_clustering=5, num_clusters=8] Cluster sizes: [20, 16, 15, 13, 13, 11, 11, 10]\n",
      "[n_clustering=5, num_clusters=9] Cluster sizes: [20, 19, 15, 12, 12, 11, 10, 5, 5]\n",
      "[n_clustering=5, num_clusters=10] Cluster sizes: [20, 15, 13, 13, 12, 11, 9, 7, 5, 4]\n",
      "[n_clustering=5, num_clusters=11] Cluster sizes: [16, 12, 12, 11, 11, 11, 9, 9, 8, 6, 4]\n",
      "[n_clustering=5, num_clusters=12] Cluster sizes: [14, 13, 12, 11, 11, 9, 8, 8, 7, 7, 5, 4]\n",
      "[n_clustering=7, num_clusters=1] Cluster sizes: [109]\n",
      "[n_clustering=7, num_clusters=2] Cluster sizes: [55, 54]\n",
      "[n_clustering=7, num_clusters=3] Cluster sizes: [44, 40, 25]\n",
      "[n_clustering=7, num_clusters=4] Cluster sizes: [36, 35, 22, 16]\n",
      "[n_clustering=7, num_clusters=5] Cluster sizes: [31, 30, 20, 15, 13]\n",
      "[n_clustering=7, num_clusters=6] Cluster sizes: [34, 20, 16, 16, 15, 8]\n",
      "[n_clustering=7, num_clusters=7] Cluster sizes: [20, 20, 17, 17, 15, 12, 8]\n",
      "[n_clustering=7, num_clusters=8] Cluster sizes: [20, 20, 15, 14, 11, 11, 10, 8]\n",
      "[n_clustering=7, num_clusters=9] Cluster sizes: [18, 15, 14, 13, 13, 11, 10, 8, 7]\n",
      "[n_clustering=7, num_clusters=10] Cluster sizes: [21, 17, 12, 12, 11, 11, 9, 7, 5, 4]\n",
      "[n_clustering=7, num_clusters=11] Cluster sizes: [16, 15, 13, 11, 11, 11, 7, 7, 6, 6, 6]\n",
      "[n_clustering=7, num_clusters=12] Cluster sizes: [14, 14, 12, 9, 9, 9, 9, 8, 8, 7, 6, 4]\n",
      "[n_clustering=9, num_clusters=1] Cluster sizes: [109]\n",
      "[n_clustering=9, num_clusters=2] Cluster sizes: [63, 46]\n",
      "[n_clustering=9, num_clusters=3] Cluster sizes: [43, 39, 27]\n",
      "[n_clustering=9, num_clusters=4] Cluster sizes: [36, 25, 25, 23]\n",
      "[n_clustering=9, num_clusters=5] Cluster sizes: [30, 23, 23, 18, 15]\n",
      "[n_clustering=9, num_clusters=6] Cluster sizes: [28, 22, 17, 15, 14, 13]\n",
      "[n_clustering=9, num_clusters=7] Cluster sizes: [24, 20, 16, 13, 13, 13, 10]\n",
      "[n_clustering=9, num_clusters=8] Cluster sizes: [23, 18, 15, 14, 11, 11, 10, 7]\n",
      "[n_clustering=9, num_clusters=9] Cluster sizes: [20, 16, 13, 12, 11, 11, 11, 10, 5]\n",
      "[n_clustering=9, num_clusters=10] Cluster sizes: [19, 18, 11, 11, 10, 10, 10, 9, 7, 4]\n",
      "[n_clustering=9, num_clusters=11] Cluster sizes: [19, 15, 14, 11, 11, 10, 8, 6, 6, 5, 4]\n",
      "[n_clustering=9, num_clusters=12] Cluster sizes: [14, 14, 13, 11, 11, 10, 8, 8, 6, 5, 5, 4]\n"
     ]
    }
   ],
   "source": [
    "# # for score_function in ['softmax', 'APS', 'RAPS']:\n",
    "# n_clustering_list = (np.array([.3, .5, .7, .9]) * n_totalcal).astype(np.int32)\n",
    "\n",
    "\n",
    "# for score_function in ['softmax', 'APS']:\n",
    "    \n",
    "#     print(f'====== score_function={score_function} ======')\n",
    "    \n",
    "#     print('Computing conformal score...')\n",
    "#     if score_function == 'softmax':\n",
    "#         scores_all = 1 - softmax_scores\n",
    "#     elif score_function == 'APS':\n",
    "#         scores_all = get_APS_scores_all(softmax_scores, randomize=True)\n",
    "#     elif score_function == 'RAPS': \n",
    "        \n",
    "#         # RAPS hyperparameters (currently using ImageNet defaults)\n",
    "#         lmbda = .01 \n",
    "#         kreg = 5\n",
    "        \n",
    "#         scores_all = get_RAPS_scores_all(softmax_scores, lmbda, kreg, randomize=True)\n",
    "#     else:\n",
    "#         raise Exception('Undefined score function')\n",
    "\n",
    "\n",
    "#     print('Splitting data...')\n",
    "#     # Split into clustering+calibration data and validation data\n",
    "#     totalcal_scores_all, totalcal_labels, val_scores_all, val_labels = split_X_and_y(scores_all, labels, n_totalcal, num_classes=num_classes, seed=0)\n",
    "\n",
    "#     for n_clustering in n_clustering_list:\n",
    "            \n",
    "#         # 0) Split data \n",
    "#         scores1_all, labels1, scores2_all, labels2 = split_X_and_y(totalcal_scores_all, \n",
    "#                                                            totalcal_labels, \n",
    "#                                                            n_clustering, \n",
    "#                                                            num_classes=num_classes, \n",
    "#                                                            seed=0)\n",
    "\n",
    "#         # 1) Compute embedding for each class\n",
    "#         embeddings = embed_all_classes(scores1_all, labels1, q=[0.5, 0.6, 0.7, 0.8, 0.9])\n",
    "\n",
    "#         # 2) Do k-means with different k's\n",
    "#         for num_clusters in np.arange(1,13):\n",
    "#             kmeans = KMeans(n_clusters=num_clusters, random_state=0, n_init=10).fit(embeddings)\n",
    "#             cluster_assignments = kmeans.labels_  \n",
    "\n",
    "#             # Print cluster sizes\n",
    "#             print(f'[n_clustering={n_clustering}, num_clusters={num_clusters}] Cluster sizes:', [x[1] for x in Counter(cluster_assignments).most_common()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4953def0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== score_function=softmax ======\n",
      "Computing conformal score...\n",
      "Splitting data...\n",
      "Best n_clustering: 5\n",
      "Best num_clusters: 3\n",
      "Cluster sizes: [88, 15, 6]\n",
      "[Clustered conformal] Class coverage gap: 3.131593054758153\n",
      "[Clustered conformal] Set size metrics {'mean': 38.618291323892166, '[.25, .5, .75, .9] quantiles': array([30., 41., 49., 55.])}\n",
      "====== score_function=APS ======\n",
      "Computing conformal score...\n",
      "Splitting data...\n",
      "Best n_clustering: 9\n",
      "Best num_clusters: 5\n",
      "Cluster sizes: [30, 23, 23, 18, 15]\n",
      "[Clustered conformal] Class coverage gap: 6.701743518999062\n",
      "[Clustered conformal] Set size metrics {'mean': 66.72654921553159, '[.25, .5, .75, .9] quantiles': array([62., 71., 76., 80.])}\n"
     ]
    }
   ],
   "source": [
    "# for score_function in ['softmax', 'APS', 'RAPS']:\n",
    "for score_function in ['softmax', 'APS']:\n",
    "    \n",
    "    print(f'====== score_function={score_function} ======')\n",
    "    \n",
    "    print('Computing conformal score...')\n",
    "    if score_function == 'softmax':\n",
    "        scores_all = 1 - softmax_scores\n",
    "    elif score_function == 'APS':\n",
    "        scores_all = get_APS_scores_all(softmax_scores, randomize=True)\n",
    "    elif score_function == 'RAPS': \n",
    "        \n",
    "        # RAPS hyperparameters (currently using ImageNet defaults)\n",
    "        lmbda = .01 \n",
    "        kreg = 5\n",
    "        \n",
    "        scores_all = get_RAPS_scores_all(softmax_scores, lmbda, kreg, randomize=True)\n",
    "    else:\n",
    "        raise Exception('Undefined score function')\n",
    "\n",
    "\n",
    "    print('Splitting data...')\n",
    "    # Split into clustering+calibration data and validation data\n",
    "    totalcal_scores_all, totalcal_labels, val_scores_all, val_labels = split_X_and_y(scores_all, labels, n_totalcal, num_classes=num_classes, seed=0)\n",
    "\n",
    "\n",
    "    qhats, preds, class_cov_gap, set_size_metrics = clustered_conformal(totalcal_scores_all, totalcal_labels,\n",
    "                                                                        alpha,\n",
    "                                                                        tune_parameters=True,\n",
    "                                                                        n_clustering=None, num_clusters=None,\n",
    "                                                                        val_scores=val_scores_all, val_labels=val_labels)\n",
    "    print('[Clustered conformal] Class coverage gap:', class_cov_gap)\n",
    "    print('[Clustered conformal] Set size metrics', set_size_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096f1204",
   "metadata": {},
   "source": [
    "# Test ad-hoc heuristic\n",
    "\n",
    "For n_totalcal=10, num_classes=108 our heuristic says 5 points for clustering, 3 clusters.\n",
    "\n",
    "\n",
    "\n",
    "This is a hard task since we only have 1,080 total calibration points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "387bd90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clustering = 5\n",
    "num_clusters = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "81379a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== score_function=softmax ======\n",
      "Computing conformal score...\n",
      "Splitting data...\n",
      "Cluster sizes: [81, 19, 7, 1, 1]\n",
      "[Clustered conformal] Class coverage gap: 2.1080014658611397\n",
      "[Clustered conformal] Set size metrics {'mean': 37.021779144955744, '[.25, .5, .75, .9] quantiles': array([29., 40., 47., 53.])}\n",
      "====== score_function=APS ======\n",
      "Computing conformal score...\n",
      "Splitting data...\n",
      "Cluster sizes: [29, 27, 27, 20, 6]\n",
      "[Clustered conformal] Class coverage gap: 2.602582134356466\n",
      "[Clustered conformal] Set size metrics {'mean': 39.76418858685417, '[.25, .5, .75, .9] quantiles': array([32., 43., 51., 56.])}\n"
     ]
    }
   ],
   "source": [
    "# for score_function in ['softmax', 'APS', 'RAPS']:\n",
    "for score_function in ['softmax', 'APS']:\n",
    "    \n",
    "    print(f'====== score_function={score_function} ======')\n",
    "    \n",
    "    print('Computing conformal score...')\n",
    "    if score_function == 'softmax':\n",
    "        scores_all = 1 - softmax_scores\n",
    "    elif score_function == 'APS':\n",
    "        scores_all = get_APS_scores_all(softmax_scores, randomize=True)\n",
    "    elif score_function == 'RAPS': \n",
    "        \n",
    "        # RAPS hyperparameters (currently using ImageNet defaults)\n",
    "        lmbda = .01 \n",
    "        kreg = 5\n",
    "        \n",
    "        scores_all = get_RAPS_scores_all(softmax_scores, lmbda, kreg, randomize=True)\n",
    "    else:\n",
    "        raise Exception('Undefined score function')\n",
    "\n",
    "\n",
    "    print('Splitting data...')\n",
    "    # Split into clustering+calibration data and validation data\n",
    "    totalcal_scores_all, totalcal_labels, val_scores_all, val_labels = split_X_and_y(scores_all, labels, n_totalcal, num_classes=num_classes, seed=0)\n",
    "\n",
    "\n",
    "    qhats, preds, class_cov_gap, set_size_metrics = _clustered_conformal(totalcal_scores_all, totalcal_labels,\n",
    "                                                                        alpha,\n",
    "                                                                        n_clustering=n_clustering, num_clusters=num_clusters,\n",
    "                                                                        val_scores=val_scores_all, val_labels=val_labels)\n",
    "    print('[Clustered conformal] Class coverage gap:', class_cov_gap)\n",
    "    print('[Clustered conformal] Set size metrics', set_size_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fcd4d923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.cache/enron_n=10/softmax/seed=0_allmetrics.pkl',\n",
       " '.cache/enron_n=10/softmax/seed=1_allmetrics.pkl',\n",
       " '.cache/enron_n=10/softmax/seed=2_allmetrics.pkl',\n",
       " '.cache/enron_n=10/softmax/seed=3_allmetrics.pkl',\n",
       " '.cache/enron_n=10/softmax/seed=4_allmetrics.pkl']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scratch\n",
    "\n",
    "import glob\n",
    "\n",
    "file_names = sorted(glob.glob('.cache/enron_n=10/softmax/*.pkl'))\n",
    "file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca6d9ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.9",
   "language": "python",
   "name": "py3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
