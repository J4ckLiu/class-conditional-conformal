{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cdfaf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob # For getting file names\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "# import seaborn as sns\n",
    "# import torch\n",
    "\n",
    "from collections import Counter\n",
    "# from gap_statistic import OptimalK\n",
    "from scipy import stats, cluster\n",
    "from sklearn.cluster import KMeans\n",
    "# from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "from utils.clustering_utils import *\n",
    "from utils.conformal_utils import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcea66a",
   "metadata": {},
   "source": [
    "Input:\n",
    "\n",
    "calibration data\n",
    "--> number of classes $K$\n",
    "--> number of examples per class (may vary). Let $n_k$ denote the number of total calibration points for clas $k$ and let $n_{min}$ denote the number of examples that the rarest class has\n",
    "\n",
    "## 1) Generate candidate parameter values \n",
    "\n",
    "**Option 1**: Use heuristic based on $K$ and $n_{min}$\n",
    "Assume $n_{min}>=5$\n",
    "\n",
    "\n",
    "O($1/\\epsilon^2$) samples are needed to distinguish between two distributions that have TV distance of $\\epsilon$ [Source:https://cstheory.stackexchange.com/questions/37607/reference-for-the-number-of-samples-needed-to-distinguish-two-probability-distri] \n",
    "\n",
    "If we assume that 5 samples are enough to distinguish between 2 clusters. Suppose that the distance between the distributions is $d$\n",
    "then we need XX samples to distinguish between 3 clusters. If we make the naive assumption that when there are 4 clusters, the TV distance between clusters is 1/2 what it was before ($d/2$), then we need 4 times as many samples as before... Which is 20. That's a lot. I think the naive assumption is also not valid. \n",
    "\n",
    "What if we instead assume that the TV distance halves when we double the number of clusters? This assumption also seems unfounded. Honestly, the best thing to do is probably run some simulations... But I should still plug in something adhoc for now. \n",
    "\n",
    "\n",
    "First time determining optimal parameters: only test n_clustering < 1/2 of n_total car\n",
    "\n",
    "Test null of one hypothesis against best parameter combination from the previous step. If we reject the null, we will try a second time to determine optimal parameters, this time allowing n_clustering to be any value up to n_totalcal. Is this overly complicated? \n",
    "\n",
    "Second time determining optimal parameters: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a1a7c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a8388a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clustering_parameters_v1(totalcal_labels):\n",
    "    '''\n",
    "    Returns a guess of good values for num_clusters and n_clustering.\n",
    "    \n",
    "    Output \n",
    "    '''\n",
    "    n = Counter(totalcal_labels).values() # Unordered\n",
    "    n_min = min(n)\n",
    "    K = len(n)\n",
    "    \n",
    "    sum_n = len(totalcal_labels)\n",
    "    \n",
    "    # TODO\n",
    "    \n",
    "    return num_clusters, n_clustering\n",
    "    \n",
    "    \n",
    "def get_clustering_parameters_v2():\n",
    "    '''\n",
    "    [WIP]\n",
    "    \n",
    "    Actually run the clustering, evaluate the gap statistic to choose the best number of clusters.\n",
    "    Still not clear how to choose n_clustering though. For now, I somewhat arbitrarily choose the\n",
    "    n_clustering with the smallest gap statistic at the optimal k. I am not sure that it makes \n",
    "    sense to compare the gap statistic across different n_clusterings since the underlying data \n",
    "    is changing \n",
    "    '''\n",
    "     # TODO\n",
    "    pass\n",
    "\n",
    "    return num_clusters, n_clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2158f3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: run some synthetic experiments to get a sense of how many samples you need to distinguish between a certain number of distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d78bb8",
   "metadata": {},
   "source": [
    "## 2) Test null hypothesis that there is one cluster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c2f6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pval_threshold = .05 \n",
    "\n",
    "# TODO: update arguments to function call \n",
    "pval = test_one_cluster_null(scores, labels, num_classes, num_clusters, \n",
    "                            num_trials=100, seed=0, print_results=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2052e4f",
   "metadata": {},
   "source": [
    "## 3) If null hypothesis is rejected, run clustered conformal.\n",
    "Else, run standard conformal on data not used for clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7d37d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pval < pval_threshold: \n",
    "    print(f'p={pval} for one cluster null hypothesis, so running Clustered Conformal')\n",
    "    # Run clustered conformal and return prediction sets\n",
    "    qhats, preds, coverage_metrics, set_size_metrics = clustered_conformal(totalcal_scores_all, totalcal_labels,\n",
    "                                                                alpha,\n",
    "                                                                n_clustering, num_clusters,\n",
    "                                                                val_scores_all=val_scores_all, val_labels=val_labels)\n",
    "    \n",
    "else:\n",
    "    print(f'p={pval} for one cluster null hypothesis, so running Standard Conformal')\n",
    "    # Run Standard Conformal and return prediction sets \n",
    "    standard_qhat = compute_qhat(totalcal_scores_all, totalcal_labels, alpha=alpha)\n",
    "    standard_preds = create_prediction_sets(val_scores_all, vanilla_qhat)\n",
    "    \n",
    "    class_cov_metrics, set_size_metrics = compute_all_metrics(val_labels, preds, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15de498d",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb8785c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = .1\n",
    "n_totalcal = 10 # Total number of calibration points (= # clustering examples + # conformal calibration examples)\n",
    "\n",
    "\n",
    "# Enron - BERT (n_train=500)\n",
    "softmax_path = \"../class-conditional-conformal-datasets/notebooks/.cache/email_softmax_bert_ntrain=500.npy\"\n",
    "labels_path = \"../class-conditional-conformal-datasets/notebooks/.cache/email_labels_bert_ntrain=500.npy\"\n",
    "\n",
    "save_folder = f'.cache/enron_n={n_totalcal}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40b471f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading softmax scores and labels...\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "print('Loading softmax scores and labels...')\n",
    "\n",
    "softmax_scores = np.load(softmax_path)\n",
    "labels = np.load(labels_path)\n",
    "\n",
    "num_classes = labels.max() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66ed4e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_all = 1 - softmax_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c30d8e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "totalcal_scores_all, totalcal_labels, val_scores_all, val_labels = split_X_and_y(scores_all, labels, n_totalcal, num_classes=num_classes, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b74fb795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(totalcal_labels).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ee81f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.9",
   "language": "python",
   "name": "py3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
